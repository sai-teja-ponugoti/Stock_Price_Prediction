{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stock_price_documentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPx9m3eyZPoEcQrPcesuG6W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sai-teja-ponugoti/Stock_Price_Prediction/blob/master/stock_price_documentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb9-vAd4X1HG",
        "colab_type": "text"
      },
      "source": [
        "# **Stock price prediction using Recurrent Neural Networks**\n",
        "\n",
        "\n",
        "*   Time Series is a collection of indexed data points based on the time during which they were collected. The data is most often recorded at regular time intervals.\n",
        "\n",
        "*   In practise, predicting future values for the time series is a very common problem. Predicting next week's weather, stock prices, tomorrow's Bitcoins price, the amount of your Chrismas sales and potential heart disease are common examples of this.\n",
        "\n",
        "\n",
        "*   Recurrent neural networks ( RNNs) may predict, or classify, the next value(s) in a series. A series is stored as a matrix, where each row is a descriptive vector of a function. The order of the rows in the matrix is of course essential.\n",
        "\n",
        "*   Time Series is just one type of a sequence. Weâ€™ll have to cut the Time Series into smaller sequences, so our RNN models can use them for training. \n",
        "\n",
        "*   Classic RNNs have memory issues (long-term dependencies). The beginning of the sequences that we use for training appears to be \"forgotten\" due to the overwhelming effect of more recent states.\n",
        "\n",
        "*   In general, these problems can be overcome by using gated RNNs. They can store information, just like having a memory, for later use. The data learns to read, write, and erase from the memory.\n",
        "\n",
        "\n",
        "*   **The two most commonly used gated RNNs are Long Short-Term Memory Networks and Gated Recurrent Unit Neural Networks**.We will try both these RNNs for or application and select one from it.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aapCoWutX1Po",
        "colab_type": "text"
      },
      "source": [
        "# **Dataset preparation and pre-processing:**\n",
        "\n",
        "**Converting data to time-series and supervised learning problem:**\n",
        "*   The dataset given to us have feature values recorded for each day.\n",
        "*   RNNs consume input in format [ batch_size, time_steps, Features ]; a 3- dimensional array.\n",
        "*   Time Steps define how many units back in time you want your network to see. In our experiment it is past 3 time frames.\n",
        "*   Features is the number of attributes used to represent each time step.In our case we are considering 4 features of each time record, these features are \"open\",\"high\",\"low\" and \"volume\". The uncessary columns datatime and \"close\" values has been removed from the dataset.\n",
        "*   In order to retain the order of dates, a index column is introuced in the dataframe, so that while plotting the prediction graph, it can be plotted from past to present, so that the graph doesn't look clumsy.\n",
        "\n",
        "\n",
        "Below function is used to prepare train and test sets from the original dataset.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# function to create train and test data sets form the given original dataset\n",
        "def create_train_test_data():\n",
        "    # reading the original csv file\n",
        "    data = pd.read_csv(\"./data/q2_dataset.csv\")\n",
        "\n",
        "    # ignoring the two unnecessary columns of the dataset\n",
        "    # ignoring Date and Close/Last columns as they dont constitue anything to \n",
        "    our problem\n",
        "    data = data[data.columns[[2, 3, 4, 5]]]\n",
        "\n",
        "    shift_window = -3\n",
        "\n",
        "    dataframe = pd.concat([data.shift(shift_window), data.shift(shift_window + \n",
        "    1), data.shift(shift_window + 2), data],\n",
        "                          axis=1)\n",
        "    dataframe.columns = ['v3', 'o3', 'h3', 'l3', 'v2', 'o2', 'h2', 'l2', 'v1', \n",
        "    'o1','h1', 'l1', 'v0', 'label', 'h0', 'l0']\n",
        "\n",
        "    # ignoring last 3 rows of the result processed data as they are not valid \n",
        "    and has Nan in them\n",
        "    dataframe = dataframe.iloc[:dataframe.shape[0] + shift_window]\n",
        "\n",
        "    # inoring extra colums volume,high,low of the last set as they are next \n",
        "    days unwanted columns\n",
        "    dataframe = dataframe[['v3', 'o3', 'h3', 'l3', 'v2', 'o2', 'h2', 'l2', \n",
        "    'v1', 'o1','h1', 'l1', 'label', ]]\n",
        "\n",
        "    dataframe['index'] = [-i for i in range(dataframe.shape[0])]\n",
        "    # shuffling the data using sklearn shuffle function\n",
        "    dataframe = shuffle(dataframe)\n",
        "\n",
        "    # splitting data to train and test parts\n",
        "    # using random state so that the results can be replicated\n",
        "    train_data, test_data = train_test_split(dataframe, test_size=0.3,\n",
        "     random_state=100)\n",
        "\n",
        "    # storing the split data into respective files\n",
        "    train_data.to_csv('./data/train_data_RNN.csv', index=False)\n",
        "    test_data.to_csv('./data/test_data_RNN.csv', index=False)\n",
        "\n",
        "    print(\"train and test data set creation is finished\")\n",
        "```\n",
        "\n",
        "**What above function is doing:**\n",
        "\n",
        "*   As the first step we read the given data set from ./data/ folder in a pandas dataframe.\n",
        "*   Then we remove the uncessary columns from the dataset.\n",
        "*   Then use a shifting window to create a dataset that consider,past 3 days data along with current days data. So in total 16 features in order from past to present.\n",
        "*   The present days \"open\" values is chosen as the labels and the remaining 3 features of current day has been removed.(samples,13) - shape of dataframe\n",
        "*   Next and index column is added to the dataframe, so that the order of dates can be tracked after shuffling the data.(samples,14) - shape of dataframe\n",
        "*   Then we use Skleran train_test_split utility function to randomly split the data into 70% train and 30% test data.\n",
        "*   This dataframes are then stored to the data folder using names \"train_data_RNN.csv\" and \"test_data_RNN.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1wLJn2pX1M4",
        "colab_type": "text"
      },
      "source": [
        "### **Pre-processing:**\n",
        "\n",
        "The data stored in train and test files is not normalized, as the unnormalized data is need to inverse the data transformation or the predicted values.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "**In train_RNN.py:**\n",
        "*    The train_data_RNN.csv\" file is read to a dataframe and index column is removed.\n",
        "*    Then the datframe is normalized using MinMax scaler to bring down all the values to (0,1). This step is taken as different columns of the data are having diffent ranges nad this affects the ability of the model to learn consistently.\n",
        "*    Then the train data features and labels are created by selecting the past 3 days features as features , and current day opening price as label.\n",
        "*    In training the order of the dates is not necessary, so the index column is not used.\n",
        "*    Then the data is reshaped to a 3D array to be able to feed to RNN network.(samples,3,4) - shape of x_train\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def load_train_data():\n",
        "    train_data = pd.read_csv(\"./data/train_data_RNN.csv\")\n",
        "    train_data = np.array(train_data)\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1)).fit(train_data[:, 0:13])\n",
        "    train_data = scaler.transform(train_data[:, 0:13])\n",
        "\n",
        "    x_train = train_data[:, 0:12]\n",
        "    x_train = np.reshape(x_train, (879, 3, 4))\n",
        "    y_train = train_data[:, 12]\n",
        "    y_train = np.asarray(y_train)\n",
        "\n",
        "    print(\"finished loading training data\")\n",
        "    return x_train, y_train\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6Cz9dBAX1K-",
        "colab_type": "text"
      },
      "source": [
        "# **Model Selection and Training:**\n",
        "\n",
        "*   The next step is to select a RNN model and try out different architures and observe the effect on training data.\n",
        "\n",
        "*   Both LSTM and GRU has been used to train and observe the results on training data.\n",
        "\n",
        "*   Function to create model with input shape is as shown below:\n",
        "\n",
        "\n",
        "```\n",
        "def create_model(input_shape_length):\n",
        "    model = Sequential()\n",
        "\n",
        "    # model.add(GRU(units=50, return_sequences=True, input_shape=(input_shape_length, 4)))\n",
        "    model.add(GRU(units=50, input_shape=(input_shape_length, 4)))\n",
        "    # model.add(Dropout(0.2))\n",
        "\n",
        "    # model.add(LSTM(units=50, return_sequences=True))\n",
        "    # model.add(Dropout(0.2))\n",
        "\n",
        "    # model.add(LSTM(units=50, return_sequences=True))\n",
        "    # model.add(Dropout(0.2))\n",
        "\n",
        "    # model.add(GRU(units=50))\n",
        "    # model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(units=1))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "    print(\"model creation is done\")\n",
        "    return model\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**The results are as shown below:**\n",
        "\n",
        "**Number of layer: count of layers excluding the outut layer**\n",
        "\n",
        "**train loss: train loss here of the normalised data, so the values are pretty low when compared with un-normalized predicted values in later section** \n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1gY7wgA1BhbuogY_3pXzf9x6jwS3VESpO)\n",
        "\n",
        "\n",
        "A simple GRU network with 1 layer and 50 units in the layer has obtained minimum train loss compared to multi layer LSTM or modles with higher units. So a GRU with single layer is slected as the imput layer of the model followed by a Dense layer with one unit to output the predicted value.\n",
        "\n",
        "**Loss function:** As the application is prediction based, Mean Squared error has been considered as the loss function.\n",
        "\n",
        "**Training loop Output:**\n",
        "\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1uFTikCvz-j86s-fIhEtx6V6StHprJnpH)\n",
        "\n",
        "\n",
        "We can consider only loss as the metric for this task. Even though this is supervised learning task , accuracy cannot be considered as a metric as the network cannot exactly predict the same value os the label.\n",
        "\n",
        "So for this loss can be considered as the metric and we can discard the accuracy.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXk0V3ZJX1EA",
        "colab_type": "text"
      },
      "source": [
        "# **Testing the trained model:**\n",
        "\n",
        "**In test_RNN.py:**\n",
        "\n",
        "**Reading the test data:** \n",
        "*   At first we read train and test files stored in data folder.\n",
        "*   Train file sis read to scale the test features using the same scaler used for train set.\n",
        "*   As the first step we sort the test set based on the index column so that data gets aligned based on the dates (past to present), then we get rid of the index column.\n",
        "*   The next step is normalising the test data using MinMax scaler that was fit on the train data.\n",
        "*   Then the test data is split in features and labels.\n",
        "*   Test features are then reshaped to a 3D array, so that it can be fed to the saved model, which exects the input to be in 3D.\n",
        "*   Return x_test,y_test and the MinMax scaler object to use to inverse the predicted value for 0-1 range to normal value.\n",
        "\n",
        "```\n",
        "def load_test_data():\n",
        "    train_data = pd.read_csv(\"./data/train_data_RNN.csv\")\n",
        "    train_data = np.array(train_data)\n",
        "    test_data = pd.read_csv(\"./data/test_data_RNN.csv\")\n",
        "    test_data = np.array(train_data)\n",
        "    test_data = test_data[np.argsort(test_data[:, 13])]\n",
        "    # test_data = test_data[:,0:13]\n",
        "\n",
        "    scaler_min_max = MinMaxScaler(feature_range=(0, 1)).fit(train_data[:, 0:13])\n",
        "    train_data = scaler_min_max.transform(train_data[:, 0:13])\n",
        "    test_data = scaler_min_max.transform(test_data[:, 0:13])\n",
        "\n",
        "    x_test = test_data[:, 0:12]\n",
        "    x_test = np.reshape(x_test, (879, 3, 4))\n",
        "    y_test = test_data[:, 12]\n",
        "    y_test = np.asarray(y_test)\n",
        "\n",
        "    return x_test, y_test, scaler_min_max\n",
        "```\n",
        "\n",
        "**Loading the Saved model and predicting the prices:**\n",
        "\n",
        "*   Then we move on to load the traied model stored in models directory.\n",
        "*   Pass the pre-processed x_test to the model and predict the values.\n",
        "*   Inverse the predicted value to normal cost range.\n",
        "*   Caluclate Mean Squared error between predicted price and actual opening price.\n",
        "*   Plot the graph to show the difference in predictions.\n",
        "\n",
        "```\n",
        "    # 1. Load your saved model\n",
        "    model = tf.keras.models.load_model(\"./models/20841154_RNN_model.h5\")\n",
        "\n",
        "    # 2. Load your testing data\n",
        "    x_test, y_test, scaler_min_max = load_test_data()\n",
        "\n",
        "    # 3. Run prediction on the test data and output required plot and loss\n",
        "    # print(type(y_test))\n",
        "    # print(predicted_stock_price)\n",
        "    predicted_stock_price = model.predict(x_test)\n",
        "    # predicted_stock_price = scaler.inverse_transform(predicted_stock_price)\n",
        "\n",
        "    y_test = (y_test * scaler_min_max.data_range_[12] + scaler_min_max.data_min_[12])\n",
        "    predicted_stock_price = (predicted_stock_price * scaler_min_max.data_range_[12] + scaler_min_max.data_min_[12])\n",
        "\n",
        "    print(\"testing Mean Squared Loss :\",mean_squared_error(y_test, predicted_stock_price))\n",
        "\n",
        "    plt.figure(figsize=(16, 12))\n",
        "    plt.plot(y_test, color='orange', label='original Stock Price')\n",
        "    plt.plot(predicted_stock_price, color='green', label='Predicted Stock Price')\n",
        "    plt.title('Stock Price Prediction',fontweight=\"bold\")\n",
        "    plt.xlabel('Time (very old to recent( left to right))',fontweight=\"bold\")\n",
        "    plt.xticks([])\n",
        "    plt.ylabel('Stock Price',fontweight=\"bold\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "```\n",
        "\n",
        "**Output of testing:**\n",
        "\n",
        "#### **testing Mean Squared Loss : 11.98907410878154**\n",
        "\n",
        "The comparision graph:\n",
        "\n",
        "<!-- ![alt text](https://drive.google.com/uc?id=1XfSuYCwV1QAOtvis7z7aSWAJ0FL6qJud) -->\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1GMbU7-X-J_dJ2KWk-zU9GlqiZkPRJ_VR)\n",
        "\n",
        "\n",
        "\n",
        "The prediction seems to work pretty well, irrespetcive of the small dataset that we have. The mean Sqaure error might decrease if we have more data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnpOqErFmj1S",
        "colab_type": "text"
      },
      "source": [
        "### **What would happen if you used more days for features:**\n",
        "\n",
        "**Considered 5 days for features:**\n",
        "\n",
        "**Results obtained: Both tarin loss and test loss has been decreased for the same model selected above**. The more previous days considered the better is the prediction.\n",
        "\n",
        "**final training loss: 0.000114** (normalized data)\n",
        "\n",
        "**final test loss: 8.2133** (un normalized data)\n",
        "\n",
        "Graph comparison:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1DLiVcq44fdnF-eYd-uKgyXFsiiuueJCL)\n"
      ]
    }
  ]
}